---
title: "Factor Model Approximation"
output:
  html_document: default
  pdf_document: default
date: "2024-01-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Model Setup

Here, I consider approaches for fitting a non-negative Poisson matrix factorization with log1p link. Specifically, for $Y \in \mathbb{N}^{n \times m}_{0}$, $U \in \mathbb{R}^{n \times K}_{+}$, and $V \in \mathbb{R}^{m \times K}_{+}$, we would like to estimate the model

\begin{align*}
y_{ij} &\sim \textrm{Poisson}(\lambda_{ij}) \quad \textrm{independently for $(i, j) \in \{1, \dots, n\} \times \{1, \dots, m\}$} \\
\log(\lambda_{ij} + 1) &= h_{ij} \\
H &= UV^{T}.
\end{align*}

The log-likelihood for this model takes the following form

\begin{align*}
\ell(U, V; Y) &= \sum_{i = 1}^{n} \sum_{j = 1}^{m} \left\{y_{ij}\log(\lambda_{ij}) - \lambda_{ij} \right\} \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{m} \left\{y_{ij}\log(\exp(h_{ij}) - 1) - \exp(h_{ij}) \right\} \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{m} \left\{y_{ij}\log\left(\exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) - 1\right) - \exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) \right\} \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{m} y_{ij}\log\left(\exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) - 1\right) - \sum_{i = 1}^{n} \sum_{j = 1}^{m} \exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right),
\end{align*}

where we have dropped all terms that are constant with respect to $U$ and $V$. For computational reasons, one may be interested in approximating the term $\sum_{i = 1}^{n} \sum_{j = 1}^{m} \exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right)$ with a linear or quadratic approximation. Previously, methods have been introduced to approximate the entire sum with a quadratic approximation. That is, assuming $\exp(x) \approx a_{0} + a_{1} x + a_{2} x^{2}$, we have

\begin{align*}
\ell(U, V; Y) & \approx \ell_{\textrm{quad_full}} (U, V; Y, a_{1}, a_{2}) \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{m} y_{ij}\log\left(\exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) - 1\right) - a_{1}\sum_{i = 1}^{n} \sum_{j = 1}^{m} \left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) - a_{2} \sum_{i = 1}^{n} \sum_{j = 1}^{m} \left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right)^{2} \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{m} y_{ij}\log\left(\exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) - 1\right) - a_{1} \sum_{k = 1}^{K} \left( \sum_{i = 1}^{n}  u_{ik} \right) \left(\sum_{j = 1}^{m}  v_{jk} \right) - a_{2} Tr\left(VU^{T}UV^{T}\right)
\end{align*}

where again we have dropped terms that do not depend on $U$ and $V$. The issue with the "full" quadratic approximation approach is that it may be difficult to find a quadratic approximation to $\exp(x)$ that is sufficiently good for the necessary range of $x$ to yield an accurate approximation to the log-likelihood. 

Instead, we are interested in only approximating the terms in the log likelihood corresponding to $y_{ij} = 0$. Intuitively, for these points $\lambda_{ij}$ will be close to $0$, and thus we only require an approximation of $\exp(x)$ that is accurate near $x = 0$. 

To ease in notation, let $I_{0} = \{(i, j) | y_{ij} = 0\}$. Then, for a quadratic approximation of $\exp(x)$, we have

\begin{align*}
\ell(U, V; Y) & \approx \ell_{\textrm{quad_sparse}} (U, V; Y, a_{1}, a_{2}) \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{m} y_{ij}\log\left(\exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) - 1\right) - \sum_{(i, j) \notin I_{0}}  \exp\left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) - a_{1}\sum_{(i, j) \in I_{0}} \left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right) - a_{2} \sum_{(i, j) \in I_{0}} \left(\sum_{k = 1}^{K} u_{ik}v_{jk}\right)^{2}.
\end{align*}

Now, since we are only interested in approximating $\exp(x)$ for small $x$, it's possible that just a linear approximation may be sufficient. Thus, as a special case of the quadratic sparse approximation, we have a linear sparse approximation. Specifically, we define $$\ell_{\textrm{lin_sparse}} (U, V; Y, a_{1}) = \ell_{\textrm{quad_sparse}} (U, V; Y, a_{1}, 0).$$

# Numerical Experiments

First, I generate a $1953 \times 1250$ matrix from the true model specified above with rank $5$. I originally generated a $2500 \times 1250$ matrix but then excluded all rows or columns with all $0$ counts.

```{r, cache=TRUE, warning=FALSE}
library(passPCA)
library(Matrix)
set.seed(1)
n <- 2500
p <- 1250
K <- 5

dat <- generate_data_simple(n, p, K)

dat$U <- dat$U[Matrix::rowSums(dat$Y) > 0, ]
dat$V <- dat$V[Matrix::colSums(dat$Y) > 0, ]
dat$Y <- dat$Y[Matrix::rowSums(dat$Y) > 0, ]
dat$Y <- dat$Y[, Matrix::colSums(dat$Y) > 0]
```

The data are approximately $89\%$ sparse. The $10$ largest values in the matrix range from $102$ to $553$.  

First, we fit the model using the exact log-likelihood.

```{r, cache=TRUE, results='hide'}
set.seed(1)
fit_exact <- fit_factor_model_log1p(dat$Y, K = 5, maxiter = 50)
```

Now, since we are interested in approximating the log-likelihood of the $0$ terms, it is useful to see the values of $\sum_{k = 1}^{K} u_{ik}v_{jk}$ for such terms. Below, we plot the density of these terms just for the zero counts (left) and just for the non-zero counts (right).

```{r, echo=FALSE}
par(mfrow = c(1, 2))
fit_exact_mean <- tcrossprod(fit_exact$U, fit_exact$V)
plot(density(fit_exact_mean[which(dat$Y == 0)], from = 0), main = "Zero Values")
plot(density(fit_exact_mean[which(dat$Y != 0)], from = 0), main = "Non-zero Values")
```

It appears that the zero counts are almost all below $0.5$. The non-zero counts are also generally concentrated near small values, but have a long right tail. 

Now, I will fit each of the different approximation methods and compare their quality.

First, we will fit the full quadratic approximation. Based on the density plots above, I will fit this approximation to be most accurate in the range of $0$ to $3$

```{r, cache=TRUE, results='hide'}
set.seed(1)
fit_quad_approx_full <- fit_factor_model_log1p_quad_approx_full(
  dat$Y,
  K = 5,
  maxiter = 50,
  approx_range = c(0, 3)
)
```

On the simulated data, the full quadratic approximation does very poorly. In fact, the true log-likelihood is actually decreasing as the algorithm runs for more iterations, as shown below:

```{r, echo=FALSE}
plot(0:50, fit_quad_approx_full$loglik_exact - sum(MatrixExtra::mapSparse(
    dat$Y, lfactorial
    )
  ),
  xlab = "Iteration",
  ylab = "Log-Likelihood"
)
```

The reason for this seems to be that the quadratic approximation works very poorly for large counts. The largest fitted poisson mean from the exact method is about $550$, where it is $6\times10^{42}$ for the quadratic approximation.

Now, we will examine the sparse approximations. First, we will try the quadratic sparse approximation. Using the density plot of the MLE above, it seems that we need this approximation to be good in the range of $0$ to $0.5$

```{r, cache=TRUE, results='hide'}
set.seed(1)
fit_quad_approx_sparse <- fit_factor_model_log1p_quad_approx_sparse(
  dat$Y,
  K = 5,
  maxiter = 50,
  approx_range = c(0, .5)
)
```

The quadratic sparse approximation does very well, with only a very small difference in log-likelihood between the quadratic sparse approximation and the exact method.

```{r, echo=FALSE}
progress_df <- data.frame(
  loglik = c(fit_exact$loglik, fit_quad_approx_sparse$loglik_exact),
  model = c(rep("Exact", 51), rep("Quad Sparse", 51)),
  iter = c(rep(0:50, 2))
)

progress_df$loglik <- progress_df$loglik - sum(
  MatrixExtra::mapSparse(dat$Y, lfactorial)
)

library(ggplot2)

progress_df$diff_from_best <- abs(progress_df$loglik - max(progress_df$loglik)) + 1

g1 <- ggplot(data = progress_df) +
  geom_line(aes(x = iter, y = loglik, color = model)) +
  geom_point(aes(x = iter, y = loglik, color = model), size = 1) +
  cowplot::theme_cowplot() +
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
  xlab("Iteration") +
  ylab("Log-Likelihood")

g2 <- ggplot(data = progress_df) +
  geom_line(aes(x = iter, y = diff_from_best, color = model)) +
  geom_point(aes(x = iter, y = diff_from_best, color = model), size = 1) +
  cowplot::theme_cowplot() +
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE), trans = "log10") +
  xlab("Iteration") +
  ylab("Distance from Best Log-Likelihood")

library(ggpubr)
ggarrange(g1, g2, common.legend = TRUE, legend = "right")
```

Per-iteration, the quadratic approximation runs about 5x faster (though this could probably be sped up with some code optimization or a sparser data set.) 

Now, we will also examine the linear approximation. Here, I will simply use the approximation $\exp(x) \approx 1 + x$ for small $x$.

```{r, results='hide', cache=TRUE}
set.seed(1)
fit_lin_approx <- fit_factor_model_log1p_lin_approx_sparse(
  dat$Y,
  K = 5,
  maxiter = 50,
  a = 1
)
```

The results of the linear approximation don't look quite as good as the quadratic approximation, though it still performs reasonably well.

```{r, echo=FALSE}
progress_df <- data.frame(
  loglik = c(fit_exact$loglik, fit_quad_approx_sparse$loglik_exact, fit_lin_approx$loglik_exact),
  model = c(rep("Exact", 51), rep("Quad Sparse", 51), rep("Lin Sparse", 51)),
  iter = c(rep(0:50, 3))
)

progress_df$loglik <- progress_df$loglik - sum(
  MatrixExtra::mapSparse(dat$Y, lfactorial)
)

library(ggplot2)

progress_df$diff_from_best <- abs(progress_df$loglik - max(progress_df$loglik)) + 1

g1 <- ggplot(data = progress_df) +
  geom_line(aes(x = iter, y = loglik, color = model)) +
  geom_point(aes(x = iter, y = loglik, color = model), size = 1) +
  cowplot::theme_cowplot() +
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
  xlab("Iteration") +
  ylab("Log-Likelihood")

g2 <- ggplot(data = progress_df) +
  geom_line(aes(x = iter, y = diff_from_best, color = model)) +
  geom_point(aes(x = iter, y = diff_from_best, color = model), size = 1) +
  cowplot::theme_cowplot() +
  scale_y_continuous(labels = function(x) format(x, scientific = TRUE), trans = "log10") +
  xlab("Iteration") +
  ylab("Distance from Best Log-Likelihood")

library(ggpubr)
ggarrange(g1, g2, common.legend = TRUE, legend = "right")
```

Comparing the fitted means between the linear sparse approximation and the exact method, things don't look very far off. The linear approximation method might be slightly underestimating the means of the very large points, but not by much.

```{r, echo=FALSE}
mle_lambda <- as.vector(exp(tcrossprod(fit_exact$U, fit_exact$V)))
lin_approx_lambda <- as.vector(exp(tcrossprod(fit_lin_approx$U, fit_lin_approx$V)))

df <- data.frame(
  mle = mle_lambda,
  lin_approx = lin_approx_lambda
)

ggplot(data = df) + 
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  geom_point(aes(x = mle, y = lin_approx), alpha = 0.25) +
  cowplot::theme_cowplot() +
  xlab("Exact MLE Fitted Mean") +
  ylab("Linear Approximation Fitted Mean")
```

# Conclusion

Overall, it seems like the linear sparse and quadratic sparse approximations are both promising approaches. The next step would be to try this on real data and examine differences in runtime / performance between the methods in depth.
