---
title: "Regression Approximation"
output: html_document
date: "2024-01-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Model Setup

Here, I consider approaches for fitting a "non-negative" Poisson regression with log1p link. Specifically, for $y \in \mathbb{N}^{n}_{0}$, $X \in \mathbb{R}^{n \times p}_{+}$, and $\beta \in \mathbb{R}^{p}_{+}$, we would like to estimate the model

\begin{align*}
y_{i} &\sim \textrm{Poisson}(\lambda_{i}) \quad \textrm{independently for $i = 1, \dots, n$} \\
\log(\lambda_{i} + 1) &= \eta_{i} \\
\eta &= X\beta.
\end{align*}

The log-likelihood for this model takes the following form

\begin{align*}
\ell(\beta; X, y) &= \sum_{i = 1}^{n} \left\{y_{i}\log(\lambda_{i}) - \lambda_{i} \right\} \\
&= \sum_{i = 1}^{n} \left\{y_{i}\log(\exp(\eta_{i}) - 1) - \exp(\eta_{i}) \right\} \\
&= \sum_{i = 1}^{n} \left\{y_{i}\log\left(\exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) - 1\right) - \exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) \right\} \\
&= \sum_{i = 1}^{n} y_{i}\log\left(\exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) - 1\right) - \sum_{i = 1}^{n} \exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right),
\end{align*}

where we have dropped all terms that are constant with respect to $\beta$. For computational reasons, one may be interested in approximating the term $\sum_{i = 1}^{n} \exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right)$ with a linear or quadratic approximation. Previously, methods have been introduced to approximate the entire sum with a quadratic approximation. That is, assuming $\exp(x) \approx a_{0} + a_{1} x + a_{2} x^{2}$, we have

\begin{align*}
\ell(\beta; X, y) & \approx \ell_{\textrm{quad_full}} (\beta; X, y, a_{1}, a_{2}) \\
&= \sum_{i = 1}^{n} y_{i}\log\left(\exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) - 1\right) - a_{1} \sum_{i = 1}^{n} \sum_{j = 1}^{p} x_{ij}\beta_{j} - a_{2} \sum_{i = 1}^{n} \left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right)^{2}  \\
&= \sum_{i = 1}^{n} y_{i}\log\left(\exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) - 1\right) - a_{1} \beta^{T} X^{T} - a_{2}\beta^{T} X^{T}X\beta,
\end{align*}

where again we have dropped terms that do not depend on $\beta$. The issue with the "full" quadratic approximation approach is that it may be difficult to find a quadratic approximation to $\exp(x)$ that is sufficiently good for the necessary range of $x$ to yield an accurate approximation to the log-likelihood. 

Instead, we are interested in only approximating the terms in the log likelihood corresponding to $y_{i} = 0$. Intuitively, for these points $\lambda_{i}$ will be close to $0$, and thus we only require an approximation of $\exp(x)$ that is accurate near $x = 0$. 

To ease in notation, let $I_{0} = \{i | y_{i} = 0\}$. Then, for a quadratic approximation of $\exp(x)$, we have

\begin{align*}
\ell(\beta; X, y) & \approx \ell_{\textrm{quad_sparse}}(\beta; X, y, a_{1}, a_{2}) \\
&= \sum_{i = 1}^{n} y_{i}\log\left(\exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) - 1\right) - \sum_{i \notin I_{0}} \exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) - a_{1} \sum_{i \in I_{0}} \sum_{j = 1}^{p} x_{ij}\beta_{j} - a_{2} \sum_{i \in I_{0}} \left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right)^{2} \\
&= \sum_{i = 1}^{n} y_{i}\log\left(\exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) - 1\right) - \sum_{i \notin I_{0}} \exp\left(\sum_{j = 1}^{p} x_{ij}\beta_{j}\right) - a_{1} \beta^{T} X_{0}^{T} - a_{2}\beta^{T} X_{0}^{T}X_{0}\beta,
\end{align*}

where $X_{0}$ is the matrix containing only the rows of $X$ corresponding to values of $y_{i}$ equal to $0$. 

Now, since we are only interested in approximating $\exp(x)$ for small $x$, it's possible that just a linear approximation may be sufficient. Thus, as a special case of the quadratic sparse approximation, we have a linear sparse approximation. Specifically, we define $$\ell_{\textrm{lin_sparse}} (\beta; X, y, a_{1}) = \ell_{\textrm{quad_sparse}} (\beta; X, y, a_{1}, 0).$$

The code required to calculate the component terms of the log-likelihood is shown below:

```{r}
get_sparse_term_loglik <- function(y_nz, X_nz, b) {
  
  sum(
    y_nz * log(exp(X_nz %*% b) - 1)
  )
  
}

get_exp_term_loglik_exact <- function(X, b) {
  
  -sum(exp(X %*% b))
  
}

get_exp_term_loglik_lin_approx <- function(X, b, a1) {
  
  -a1 * sum(colSums(X) * b)
  
}

get_exp_term_loglik_quad_approx <- function(X, b, a1, a2) {
  
  get_exp_term_loglik_lin_approx(X, b, a1) - a2 * (
    t(b) %*% crossprod(X) %*% b
  )
  
}
```

Then, putting these functions together, we can define the log-likelihood for each model above.

```{r}
# exact log-likelihood for regression
get_exact_loglik <- function(b, X, y) {
  
  y_nz_idx <- which(y != 0)
  ll <- get_sparse_term_loglik(
    y[y_nz_idx], X[y_nz_idx, , drop = FALSE], b
  ) + get_exp_term_loglik_exact(X, b)
  return(ll)
  
}

# log-likelihood approximating only sparse terms with linear approximation
get_lin_sparse_approx_loglik <- function(b, X, y, a1) {
  
  y_nz_idx <- which(y != 0)
  y_z_idx <- which(y == 0)
  ll <- get_sparse_term_loglik(
    y[y_nz_idx], X[y_nz_idx, , drop = FALSE], b
  ) + get_exp_term_loglik_exact(
    X[y_nz_idx, , drop = FALSE], b
  ) + get_exp_term_loglik_lin_approx(
    X[y_z_idx, , drop = FALSE], b, a1
  )
  return(ll)
  
}

# log-likelihood approximating only sparse terms with quadratic approximation
get_quad_sparse_approx_loglik <- function(b, X, y, a1, a2) {
  
  y_nz_idx <- which(y != 0)
  y_z_idx <- which(y == 0)
  ll <- get_sparse_term_loglik(
    y[y_nz_idx], X[y_nz_idx, , drop = FALSE], b
  ) + get_exp_term_loglik_exact(
    X[y_nz_idx, , drop = FALSE], b
  ) + get_exp_term_loglik_quad_approx(
    X[y_z_idx, , drop = FALSE], b, a1, a2
  )
  return(ll)
  
}

# log-likelihood approximating ALL terms with quadratic approximation
get_quad_full_approx_loglik <- function(b, X, y, a1, a2) {
  
  y_nz_idx <- which(y != 0)
  ll <- get_sparse_term_loglik(
    y[y_nz_idx], X[y_nz_idx, , drop = FALSE], b
  ) + get_exp_term_loglik_quad_approx(
    X, b, a1, a2
  )
  return(ll)
  
}
```

# Fidelity of the Approximation 

An important aspect of all of the approximations above is selecting the polynomial itself. Ideally, this polynomial would match $\exp(x)$ as closely as possible in a reasonable range. 

Since we are only approximating points for which $y_{i} = 0$, it is useful to determine the range of plausible values of $\lambda_{i}$ for these points.

```{r, echo=FALSE}
lambda_vec <- seq(0.01, 7, .01)
prob_0 <- exp(-lambda_vec)

library(ggplot2)
df <- data.frame(
  lambda = lambda_vec,
  prob_0 = prob_0
)

ggplot(data = df) +
  geom_line(aes(x = lambda, y = prob_0)) +
  cowplot::theme_cowplot() +
  xlab("lambda") +
  ylab("probability of y = 0") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_x_continuous(breaks = 0:7) +
  scale_y_continuous(breaks = seq(0, 1, .2))
```

Based on this plot, it appears that a good approximation to $\exp(\lambda)$ in the range of $[0, 4]$ is probably sufficient. Examining the link function, this requires a good approximation of $\exp(x)$ in the range $[0, \log(5)]$. 

Using Chebyshev polynomials, we examine the fidelity of both the linear and the quadratic approximations below:

```{r, echo=FALSE}
par(mfrow = c(1, 2))
ans <- pracma::polyApprox(exp,a = 0,b = log(5),n = 2)
f <- ans$f
x <- seq(0,log(8),length.out = 1000)
plot(x,exp(x),type = "l",lwd = 1.5, main = "Quadratic Approximation")
lines(x,f(x),lwd = 1.5,col = "red",lty = "dashed")
legend(0.1, 7, legend=c("Actual", "Approximate"),
       col=c("black", "red"), lty=1:2, cex=0.8)

ans <- pracma::polyApprox(exp,a = 0,b = log(5),n = 1)
f <- ans$f
x <- seq(0,log(8),length.out = 1000)
plot(x,exp(x),type = "l",lwd = 1.5, main = "Linear Approximation")
lines(x,f(x),lwd = 1.5,col = "red",lty = "dashed")
legend(0.1, 7, legend=c("Actual", "Approximate"),
       col=c("black", "red"), lty=1:2, cex=0.8)
```

Based on the plots above, it seems that the quadratic approximation is very good in the plausible range for $\lambda$. It also appears that the linear approximation might be okay, but it is hard to say precisely. 

# Simulations

To test the different approximations, I simulated data under the model specified above and compared maximum likelihood estimates. 

First, I simulated data with just one regression parameter, as below:

```{r}
set.seed(1)
n <- 300
p <- 1

x <- numeric(n)
x[1:225] <- abs(rnorm(n = 225, sd = .25))
x[226:300] <- abs(rnorm(n = 75, sd = 1.75))
b <- matrix(data = 1)
X <- matrix(data = x, ncol = 1)

lambda <- exp(X %*% b) - 1
y <- rpois(n, lambda)
```

Now, we solve for the optimal regression parameter. For the full quadratic approximation, I am using a polynomial approximated on the interval $[0, \log(8)]$.

```{r}
exact_sol <- optimize(
  get_exact_loglik,
  lower = 0,
  upper = 10,
  maximum = TRUE,
  X = X,
  y = y
)$maximum

# now, need to determine the lengths of the approximation intervals
approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(5),
  n = 1
)

a0_linear <- approx_poly$p[2]
a1_linear <- approx_poly$p[1]

lin_sparse_approx_sol <- optimize(
  get_lin_sparse_approx_loglik,
  lower = 0,
  upper = 10,
  maximum = TRUE,
  X = X,
  y = y,
  a1 = a1_linear
)$maximum

approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(5),
  n = 2
)

a0_quad <- approx_poly$p[3]
a1_quad <- approx_poly$p[2]
a2_quad <- approx_poly$p[1]

quad_sparse_approx_sol <- optimize(
  get_quad_sparse_approx_loglik,
  lower = 0,
  upper = 10,
  maximum = TRUE,
  X = X,
  y = y,
  a1 = a1_quad,
  a2 = a2_quad
)$maximum

approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(8),
  n = 2
)

a0_quad_full <- approx_poly$p[3]
a1_quad_full <- approx_poly$p[2]
a2_quad_full <- approx_poly$p[1]

quad_full_approx_sol <- optimize(
  get_quad_full_approx_loglik,
  lower = 0,
  upper = 10,
  maximum = TRUE,
  X = X,
  y = y,
  a1 = a1_quad_full,
  a2 = a2_quad_full
)$maximum

```

The two sparse approximation methods appear to give similar results to the true maximum likelihood estimate, as can be seen below:

```{r, echo=FALSE}
b_interval <- seq(0.6, 1.4, .01)

b_exact_obj <- c()

for (b in b_interval) {
  
  b_exact_obj <- c(
    b_exact_obj, 
    get_exact_loglik(b, X, y)
  )
  
}

b_quad_obj <- c()

for (b in b_interval) {
  
  b_quad_obj <- c(
    b_quad_obj, 
    get_quad_sparse_approx_loglik(
      b, X, y, a1_quad, a2_quad
    )
  )
  
}

b_lin_obj <- c()

for (b in b_interval) {
  
  b_lin_obj <- c(
    b_lin_obj, 
    get_lin_sparse_approx_loglik(
      b, X, y, a1_linear
    )
  )
  
}

lik_df <- data.frame(
  b = rep(b_interval, 3),
  objective = c(
    b_exact_obj,
    b_quad_obj - n * a0_quad,
    b_lin_obj - n * a0_linear
  ),
  algorithm = c(
    rep("exact", length(b_interval)),
    rep("quad approx", length(b_interval)),
    rep("lin approx", length(b_interval))
  )
)

library(ggplot2)

ggplot(data = lik_df) +
  geom_line(aes(x = b, y = objective, color = algorithm, linetype = algorithm)) + 
  geom_point(
    aes(
      x = quad_sparse_approx_sol,
      y = get_quad_sparse_approx_loglik(
        quad_sparse_approx_sol, X, y, a1_quad, a2_quad
      ) - n * a0_quad,
    ),
    color = "blue",
    shape = 4
  ) + 
    geom_point(
    aes(
      x = lin_sparse_approx_sol,
      y = get_lin_sparse_approx_loglik(
        lin_sparse_approx_sol, X, y, a1_linear
      ) - n * a0_linear,
    ),
    color = "forestgreen",
    shape = 4
  ) + 
      geom_point(
    aes(
      x = exact_sol,
      y = get_exact_loglik(
        exact_sol, X, y
      ),
    ),
    color = "red",
    shape = 4
  ) +
  scale_color_manual(
    values = c("red", "forestgreen", "blue")
  ) + cowplot::theme_cowplot()
```

However, the full quadratic approximation is very far off. The optimum for that method is approximately $3.4$. It is difficult to show this on a plot. 

Now, I want to see how well these methods will generalize to higher dimension regression coefficients. I simulated data using the setup below:

```{r}
set.seed(1)
n <- 500
p <- 5

X <- matrix(
  data = 0, nrow = n, ncol = p
)

X[, 1] <- abs(rnorm(n, sd = .5))
X[, 2] <- abs(rnorm(n, sd = 1))
X[, 3] <- rexp(n, rate = 3)
X[, 4] <- rgamma(n, shape = 3, rate = 3)
X[1:460, 5] <- abs(rnorm(460, sd = .01))
X[461:500, 5] <- rexp(40, 1)

b <- c(0, 0, .1, .3, .6)

lambda <- exp(X %*% b) - 1
y <- rpois(n, lambda)
```

Now, again we optimize each of the approximations.

```{r}
init = abs(rnorm(5))

exact_sol <- optim(
  par = init,
  fn = get_exact_loglik,
  X = X,
  y = y,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par

# now, need to determine the lengths of the approximation intervals
approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(5),
  n = 1
)

a0_linear <- approx_poly$p[2]
a1_linear <- approx_poly$p[1]

lin_sparse_approx_sol <- optim(
  par = init,
  fn = get_lin_sparse_approx_loglik,
  X = X,
  y = y,
  a1 = a1_linear,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par


approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(5),
  n = 2
)

a0_quad <- approx_poly$p[3]
a1_quad <- approx_poly$p[2]
a2_quad <- approx_poly$p[1]

quad_sparse_approx_sol <- optim(
  par = init,
  fn = get_quad_sparse_approx_loglik,
  X = X,
  y = y,
  a1 = a1_quad,
  a2 = a2_quad,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par

approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(8),
  n = 2
)

a1_quad_full <- approx_poly$p[2]
a2_quad_full <- approx_poly$p[1]

quad_full_approx_sol <- optim(
  par = init,
  fn = get_quad_full_approx_loglik,
  X = X,
  y = y,
  a1 = a1_quad,
  a2 = a2_quad,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par
```

Now, we can examine the performance of the different methods:


```{r, echo=FALSE}
ests_df <- data.frame(
  param = as.factor(rep(c("b1", "b2", "b3", "b4", "b5"), 5)),
  est = c(
    exact_sol,
    lin_sparse_approx_sol,
    quad_sparse_approx_sol,
    quad_full_approx_sol,
    b
  ), 
  algorithm = c(
    rep("mle", 5),
    rep("Lin Sparse Approx", 5),
    rep("Quad Sparse Approx", 5),
    rep("Quad Full Approx", 5),
    rep("truth", 5)
  )
)

ggplot(data = ests_df) +
  geom_point(
    aes(x = param, y = est, color = algorithm), shape = 8, size = 2
  ) +
  xlab("Parameter") +
  ylab("Estimate") +
  cowplot::theme_cowplot()
```

```{r, include=FALSE}
library(dplyr)
```


```{r, echo=FALSE}
knitr::kable(
  reshape(
    ests_df, 
    timevar = "param", 
    idvar = "algorithm",
    direction = "wide"
    ) %>% 
 mutate_if(is.numeric, round, digits = 2) %>%
   rename(b1 = est.b1, b2 = est.b2, b3 = est.b3, b4 = est.b4, b5 = est.b5)
)
```

Overall, it seems that the quadratic sparse approximation and linear sparse approximation both perform quite well. The full quadratic approximation seems to perform okay, though it's estimate for b5 is quite off. 

Finally, I examine one last regression problem, this time with the number of parameters set to $10$ and the values of $\lambda$ all quite small.

```{r}
set.seed(1)
n <- 5000
p <- 10
X <- matrix(
  data = abs(rnorm(n * p, sd = .15)),
  nrow = n,
  ncol = p
)
b <- abs(rnorm(p, sd = 0.75))
b[c(1, 2, 3, 4, 7, 8, 10)] <- 0
lambda <- exp(X %*% b) - 1
y <- rpois(n, lambda)
init <- abs(rnorm(p, sd = 0.5))
```

Again, we estimate the regression coefficients under the different models:

```{r}
exact_sol <- optim(
  par = init,
  fn = get_exact_loglik,
  X = X,
  y = y,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par

approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(5),
  n = 1
)

a0_linear <- approx_poly$p[2]
a1_linear <- approx_poly$p[1]

lin_sparse_approx_sol <- optim(
  par = init,
  fn = get_lin_sparse_approx_loglik,
  X = X,
  y = y,
  a1 = a1_linear,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par

approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(8),
  n = 2
)

a0_quad <- approx_poly$p[3]
a1_quad <- approx_poly$p[2]
a2_quad <- approx_poly$p[1]

quad_sparse_approx_sol <- optim(
  par = init,
  fn = get_quad_sparse_approx_loglik,
  X = X,
  y = y,
  a1 = a1_quad,
  a2 = a2_quad,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par

approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = log(8),
  n = 2
)

a1_quad_full <- approx_poly$p[2]
a2_quad_full <- approx_poly$p[1]

quad_full_approx_sol <- optim(
  par = init,
  fn = get_quad_full_approx_loglik,
  X = X,
  y = y,
  a1 = a1_quad,
  a2 = a2_quad,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par
```

We visualize the results below:

```{r, echo=FALSE}
ests_df <- data.frame(
  param = factor(rep(paste0("b", 1:10), 5), levels = paste0("b", 1:10)),
  est = c(
    exact_sol,
    lin_sparse_approx_sol,
    quad_sparse_approx_sol,
    quad_full_approx_sol,
    b
  ), 
  algorithm = c(
    rep("mle", 10),
    rep("Lin Sparse Approx", 10),
    rep("Quad Sparse Approx", 10),
    rep("Quad Full Approx", 10),
    rep("truth", 10)
  )
)

ggplot(data = ests_df) +
  geom_point(
    aes(x = param, y = est, color = algorithm), shape = 8, size = 2
  ) +
  xlab("Parameter") +
  ylab("Estimate") +
  cowplot::theme_cowplot()
```


```{r, echo=FALSE}
knitr::kable(
  reshape(
    ests_df, 
    timevar = "param", 
    idvar = "algorithm",
    direction = "wide"
    ) %>% 
 mutate_if(is.numeric, round, digits = 2) %>%
   rename(b1 = est.b1, b2 = est.b2, b3 = est.b3, b4 = est.b4, b5 = est.b5, b6 = est.b6, b7 = est.b7, b8 = est.b8, b9 = est.b9, b10 = est.b10)
)
```

Here, we can see that the linear approximation actually does not perform very well. I believe that this is because the linear predictor values $X\beta$, are all concentrated in a very small region between $0$ and $1$, and the linear approximation was created for a larger range. 

To test this theory, I narrowed the approximation range to $[0, 1]$, which seems to have improved the approximation, as shown below.

```{r}
approx_poly <- pracma::polyApprox(
  exp,
  a = 0,
  b = 1,
  n = 1
)

a0_linear <- approx_poly$p[2]
a1_linear <- approx_poly$p[1]

lin_sparse_approx_sol <- optim(
  par = init,
  fn = get_lin_sparse_approx_loglik,
  X = X,
  y = y,
  a1 = a1_linear,
  lower = rep(0, 5),
  control = list(fnscale = -1),
  method = "L-BFGS-B"
)$par

names(lin_sparse_approx_sol) <- paste0("b", 1:10)

print(round(lin_sparse_approx_sol, 2))
```

# Conclusion

In simulations, it seems like the sparse approximation methods work reasonably well, and certainly better in general than the full approximation to the log-likelihood. The linear approximation is definitely more sensitive to the range of the approximation. In practice, it may be possible to modify the approximation slightly based on the data (e.g. if all of the counts are very small, it may be best to use a smaller approximation range). The next step is to start applying this to matrix factorization problems.
